function [Priors Mu Sigma fval]=SEDS_Solver_MSE(Priors_0,Mu_0,Sigma_0,Data,options)
%
% SEDS optimization toolbox: version 1.9 issued on 05 August 2011
%
% This function finds a locally optimal value of a Gaussian Mixture Model
% under the constraint of ensuring its global asymptotic stability.
%
% This function should not be used directly. Please use SEDS_Solver
% function with the option options.objective = 'mse' instead.
%
% The function can be called using:
%       [Priors Mu Sigma]=SEDS_Solver_MSE(Priors_0,Mu_0,Sigma_0,Data,options)
%
% to also pass a structure of desired options.
%
% Important NOTE: Both the demonstration data, and the model estimation
% should be in the target frame of reference. In other words, this codes
% assumes that the target is at the origin!
%
% Inputs -----------------------------------------------------------------
%
%   o Priors_0:  1 x K array representing an initial guess for prior
%                probabilities of the K GMM components.
%
%   o Mu_0:      2d x K array representing an initial guess for centers of
%                the K GMM components.
%
%   o Sigma_0:   2d x 2d x K array representing an initial guess for
%                covariance matrices of the K GMM components.
%
%   o Data:      A 2d x N_Total matrix containing all demonstration data points.
%                Rows 1:d corresponds to trajectories and the rows d+1:2d
%                are their first time derivatives. Each column of Data stands
%                for a datapoint. All demonstrations are put next to each other 
%                along the second dimension. For example, if we have 3 demos
%                D1, D2, and D3, then the matrix Data is:
%                                 Data = [[D1] [D2] [D3]]
%
%   o options: A structure to set the optional parameters of the solver.
%              The following parameters can be set in the options:
%       - .tol_mat_bias:     a very small positive scalar to avoid
%                            instabilities in Gaussian kernel [default: 10^-15]
%
%       - .tol_stopping:     A small positive scalar defining the stoppping
%                            tolerance for the optimization solver [default: 10^-10]
%
%       - .i_max:            maximum number of iteration for the solver [default: i_max=1000]
%
%       - .objective:        'likelihood': use likelihood as criterion to
%                            optimize parameters of GMM
%                            'mse': use mean square error as criterion to
%                            optimize parameters of GMM
%                            [default: 'mse']
%
%       - .display:          An option to control whether the algorithm
%                            displays the output of each iterations [default: true]
%
%       - .perior_opt:       Shall the solver optimize priors? This is an
%                            option given to the user if s/he wishes not to
%                            optimize the priors [default: true]
%
%       - .mu_opt:           Shall the solver optimize centers? This is an
%                            option given to the user if s/he wishes not to
%                            optimize the centers Mu [default: true]
%
%       - .sigma_x_opt:      Shall the solver optimize Sigma_x? This is an
%                            option given to the user if s/he wishes not to
%                            optimize the Sigma_x [default: true]
%
%       - .normalization:    Activating the normalization options usually 
%                            improves the learning performance, especially
%                            for the case where the variance of the data is
%                            big along some dimensions. This option is not
%                            completely tested! [default: false]
%
% Outputs ----------------------------------------------------------------
%
%   o Priors:  1 x K array representing the prior probabilities of the K GMM 
%              components.
%
%   o Mu:      2d x K array representing the centers of the K GMM components.
%
%   o Sigma:   2d x 2d x K array representing the covariance matrices of the 
%              K GMM components.
%
%  NOTE: The final model is in the target frame of reference.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%    Copyright (c) 2010 S. Mohammad Khansari-Zadeh, LASA Lab, EPFL,   %%%
%%%          CH-1015 Lausanne, Switzerland, http://lasa.epfl.ch         %%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% The program is free for non-commercial academic use. Please contact the
% author if you are interested in using the software for commercial purposes.
% The software must not be modified or distributed without prior permission
% of the authors. Please acknowledge the authors in any academic publications
% that have made use of this code or part of it. Please use this BibTex
% reference:
% 
% S. M. Khansari-Zadeh and A. Billard, "Learning Stable Non-Linear Dynamical 
% Systems with Gaussian Mixture Models", IEEE Transaction on Robotics, 2011.
%
% To get latest upadate of the software please visit
%                          http://lasa.epfl.ch/khansari
%
% Please send your feedbacks or questions to:
%                           mohammad.khansari_at_epfl.ch

%% initializing ...

d = size(Sigma_0,1)/2; %dimension of the model
K = size(Sigma_0,3); %number of Gaussian functions

%% Optimization

%transforming the GMM model into a vector of optimization parameters
p0 = GMM_2_Parameters(Priors_0,Mu_0,Sigma_0,d,K);

obj_handle = @(p) obj(p,Data,d,K,options);
ctr_handle = @(p) ctr_MSE_eigenvalue(p,d,K);

% Running the optimization
if options.display
    str = 'iter';
else
    str = 'off';
end

% Options for NLP Solvers
optNLP = optimset( 'Algorithm', 'interior-point', 'LargeScale', 'off',...
    'GradObj', 'on', 'GradConstr', 'on', 'DerivativeCheck', 'off', ...
    'Display', 'iter', 'TolX', options.tol_stopping, 'TolFun', options.tol_stopping, 'TolCon', 1e-12, ...
    'MaxFunEval', 200000, 'MaxIter', options.max_iter, 'DiffMinChange', ...
    options.tol_stopping, 'Hessian','off','display',str);

% Solve fully-discretized optimal control problem
[popt fval] = fmincon(obj_handle, p0,[],[],[],[],[],[],ctr_handle,optNLP);

% transforming back the optimization parameters into the GMM model
[Priors Mu Sigma] = Parameters_2_GMM(popt,d,K,options);
Priors = Priors/sum(Priors);
% Sigma(d+1:2*d,d+1:2*d,:) = Sigma_0(d+1:2*d,d+1:2*d,:);

% Just to check if every thing goes well :)
c = ctr_handle(popt);
check_constraints(c,d,K);


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
function [J, dJ]=obj(p,Data,d,K,options)
% This function computes the derivative of the likelihood objective function
% w.r.t. optimization parameters.
nData = size(Data,2);
[Priors Mu Sigma A] = shape_DS(p,d,K);

x = Data(1:d,:);
[xd, tmp, h]= GMR(Priors,Mu,Sigma,x,1:d,d+1:2*d);
% h(h==0) = realmin;

dJdxd = xd-Data(d+1:2*d,:); %derivative of J w.r.t. xd
dJ = zeros(size(p));
if nargout > 1 
    % computing dJ
    rSrs = zeros(d,d);
    for k=1:K
        %since we use these eq. a lot. It is better to compute them once
        sum_tmp = sum((A(:,:,k)*x-xd).*dJdxd); 
        tmp_x = x - repmat(Mu(1:d,k),1,nData);
        invSigma_x = eye(d)/Sigma(1:d,1:d,k);

        % Sensitivity of Obj w.r.t. Priors^k
        if options.perior_opt
            dJ(k) = exp(-p(k))*Priors(k)*sum(h(:,k)'.*sum_tmp); %derivative of xd w.r.t. priors(k)
        end


        % Sensitivity of Obj w.r.t. Mu
        if options.mu_opt
    %         dJ(K+(k-1)*d+1:K+k*d) = (h(:,k)'.*sum_tmp)*((x-repmat(Mu(1:d,k),1,nData))'/Sigma(1:d,1:d,k));
            dJ(K+(k-1)*d+1:K+k*d) = invSigma_x*tmp_x*(h(:,k).*sum_tmp');
        end

        % Sensitivity of Obj w.r.t. Sigma
        i_c=0;
        i_a = d*(d+1)/2;
        for i=1:d
            for j=1:d
                if options.sigma_x_opt && j>=i %finding dJ w.r.t. Sigma_x parameters
                    i_c = i_c + 1;
                    rSrs = rSrs *0;
                    rSrs(j,i)=1;
                    rSrs = rSrs + rSrs';

                    dJ(K+K*d+(k-1)*d*(3*d+1)/2+i_c) = 0.5*sum(...
                          (sum((invSigma_x*rSrs*invSigma_x*tmp_x).*tmp_x) + ... %derivative w.r.t. Sigma in exponential
                              -trace(invSigma_x*rSrs)).*sum_tmp.*h(:,k)'); %derivative with respect to det Sigma which is in the numenator

                end

                %finding dJ w.r.t. Sigma_xdx parameters
                rSrs = rSrs *0;
                rSrs(j,i)=1;
                i_a = i_a + 1;
                dJ(K+K*d+(k-1)*d*(3*d+1)/2+i_a) = sum(sum((rSrs*x).*dJdxd).*h(:,k)');  %derivative of A
                
            end
        end
    end
end
J = 0.5*sum(sum(dJdxd.*dJdxd))/nData;
dJ = dJ/nData;
if options.normalization
    J = J/options.Wn(d+1,d+1)^2;
    dJ = dJ/options.Wn(d+1,d+1)^2;
end


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
function [Priors Mu Sigma A C L_x] = shape_DS(p,d,K)
% transforming the column of parameters into Priors, Mu, and Sigma
L_x = zeros(d,d,K);
Mu = zeros(2*d,K);
Sigma = zeros(2*d,2*d,K);
A = zeros(d,d,K);

if K==1
    Priors = 1;
else
    Priors = 1./(1+exp(-p(1:K)));
end

Mu(1:d,:) = reshape(p(K+1:K+d*K),d,K);

i_c = K+d*K+1;
for k=1:K
    for i=1:d
        L_x(i:d,i,k) = p(i_c:i_c+d-i);
        i_c = i_c + d - i + 1;
    end
    A(:,:,k) = reshape(p(i_c:i_c+d^2-1),d,d);
    i_c = i_c + d^2;
    Sigma(1:d,1:d,k) = L_x(:,:,k) + L_x(:,:,k)';
    [v lambda] = eig(Sigma(1:d,1:d,k));
    if any(diag(lambda)< 0)
        lambda(lambda<=0) = 1e-6;
        Sigma(1:d,1:d,k) = v'*lambda*v;
    end

    Sigma(d+1:2*d,1:d,k) = A(:,:,k)*Sigma(1:d,1:d,k);
    Mu(d+1:2*d,k) = A(:,:,k)*Mu(1:d,k);
end

C = reshape(p(K+K*d+K*(d/2*(d+1)+d^2)+1:end),d,d);


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
function p0 = GMM_2_Parameters(Priors,Mu,Sigma,d,K)
% transforming optimization parameters into a column vector
p0 = [-log(1./Priors(:)-1);reshape(Mu(1:d,:),[],1)];
for k=1:K
    tmp_mat = Sigma(1:d,1:d,k) - diag(diag(Sigma(1:d,1:d,k))/2);
    for i=1:d
        p0 = [p0;tmp_mat(i:d,i)]; %#ok<*AGROW>
    end
    A_tmp = Sigma(d+1:2*d,1:d,k)/Sigma(1:d,1:d,k);
    p0 = [p0;reshape(A_tmp,[],1)];
end
C = eye(d);
p0 = [p0;C(:)];

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
function [Priors Mu Sigma] = Parameters_2_GMM(popt,d,K,options)
% transforming the column of parameters into Priors, Mu, and Sigma
[Priors Mu Sigma] = shape_DS(popt,d,K);
if options.normalization
    for k=1:K
        Sigma(:,:,k) = options.Wn\Sigma(:,:,k)/options.Wn;
        Mu(1:d,k) = options.Wn(1:d,1:d)\Mu(1:d,k);
        Mu(d+1:2*d,k) = Sigma(d+1:2*d,1:d,k)/Sigma(1:d,1:d,k)*Mu(1:d,k);
        Sigma(1:d,d+1:2*d,k) = Sigma(d+1:2*d,1:d,k)';
    end
end

function check_constraints(c,d,K)
if all(c<0)
    disp('Optimization finished successfully.')
    disp(' ')
    disp(' ')
else
    c = reshape(c,d,[]);
    [i,j] = find(c >= 0);
    c(:,K+1:end) = -c(:,K+1:end);
    
    disp(' ')
    disp(' ')
    disp('Optimization did not reach to an optimal point.')
    disp('Some constraints were slightly violated.')
    disp('The error is due to the matlab fmincon function.')
    disp('For debugging purpose:')
    ind = j(j<=K)';
    if ~isempty(ind)
        disp('  error in A^k:')
        disp([ind;c(:,ind)])
    end
    ind = j(j>K+1)';
    if ~isempty(ind)
        disp('  error in Sigma^k:')
        disp([ind-K-1;c(:,ind)])
    end
    ind = j(j==K+1)';
    if ~isempty(ind)
        disp('  error in Lyapunov:')
        disp([c(:,ind)])
    end
    disp(' ')
end